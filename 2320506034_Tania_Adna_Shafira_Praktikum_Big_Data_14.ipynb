{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "099562db"
      },
      "source": [
        "## Objectives:\n",
        "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
        "- Build and evaluate models using real-world datasets.\n",
        "- Explore techniques like feature engineering and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77df771a"
      },
      "source": [
        "## Introduction to Spark MLlib\n",
        "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ae225b",
        "outputId": "01cc9cab-4547-4a8a-eb74-739a6f01459c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b266267",
        "outputId": "003bf39a-cb1c-40d0-8770-25e3042e706a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
        "columns = ['ID', 'Features', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Assuming 'Features' column contains arrays like [2.0, 3.0]\n",
        "# We need to extract the individual elements as separate columns first.\n",
        "\n",
        "# Extract elements from 'Features' array into individual columns\n",
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn('Feature1', col('Features').getItem(0))\n",
        "df = df.withColumn('Feature2', col('Features').getItem(1))\n",
        "\n",
        "# Create a VectorAssembler to convert the extracted features to a VectorUDT\n",
        "assembler = VectorAssembler(inputCols=['Feature1', 'Feature2'], outputCol='Features_vec')\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Train logistic regression model, using the new 'Features_vec' column\n",
        "lr = LogisticRegression(featuresCol='Features_vec', labelCol='Label')\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9066e04",
        "outputId": "ced2f5b0-142b-4a97-f89b-eb15b617cc58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([5.33333333]), array([15.])]\n"
          ]
        }
      ],
      "source": [
        "# Practice: KMeans Clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, [1.0, 1.0]), (2, [5.0, 5.0]), (3, [10.0, 10.0]), (4, [15.0, 15.0])]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn('Feature1', col('Features').getItem(0))\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['Feature1'], outputCol='Features_vec')\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features_vec', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a60a8d7e"
      },
      "source": [
        "## Homework\n",
        "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
        "- Build a classification model using Spark MLlib and evaluate its performance.\n",
        "- Explore hyperparameter tuning using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV-gGHID2v5T",
        "outputId": "c38d5b63-224e-42e1-c4ee-563db6155265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Non-Null Count   Dtype  \n",
            "---  ------          --------------   -----  \n",
            " 0   Region          100000 non-null  object \n",
            " 1   Country         100000 non-null  object \n",
            " 2   Item Type       100000 non-null  object \n",
            " 3   Sales Channel   100000 non-null  object \n",
            " 4   Order Priority  100000 non-null  object \n",
            " 5   Order Date      100000 non-null  object \n",
            " 6   Order ID        100000 non-null  int64  \n",
            " 7   Ship Date       100000 non-null  object \n",
            " 8   Units Sold      100000 non-null  int64  \n",
            " 9   Unit Price      100000 non-null  float64\n",
            " 10  Unit Cost       100000 non-null  float64\n",
            " 11  Total Revenue   100000 non-null  float64\n",
            " 12  Total Cost      100000 non-null  float64\n",
            " 13  Total Profit    100000 non-null  float64\n",
            "dtypes: float64(5), int64(2), object(7)\n",
            "memory usage: 10.7+ MB\n",
            "None\n",
            "\n",
            "Statistik Deskriptif:\n",
            "           Order ID     Units Sold     Unit Price      Unit Cost  \\\n",
            "count  1.000000e+05  100000.000000  100000.000000  100000.000000   \n",
            "mean   5.503956e+08    5001.446170     266.703989     188.019711   \n",
            "std    2.593219e+08    2884.575424     216.940081     175.706023   \n",
            "min    1.000089e+08       1.000000       9.330000       6.920000   \n",
            "25%    3.260464e+08    2505.000000     109.280000      56.670000   \n",
            "50%    5.477185e+08    5007.000000     205.700000     117.110000   \n",
            "75%    7.750785e+08    7495.250000     437.200000     364.690000   \n",
            "max    9.999965e+08   10000.000000     668.270000     524.960000   \n",
            "\n",
            "       Total Revenue    Total Cost  Total Profit  \n",
            "count   1.000000e+05  1.000000e+05  1.000000e+05  \n",
            "mean    1.336067e+06  9.419755e+05  3.940912e+05  \n",
            "std     1.471768e+06  1.151828e+06  3.795986e+05  \n",
            "min     1.866000e+01  1.384000e+01  4.820000e+00  \n",
            "25%     2.797533e+05  1.629283e+05  9.590000e+04  \n",
            "50%     7.898916e+05  4.679374e+05  2.836575e+05  \n",
            "75%     1.836490e+06  1.209475e+06  5.683841e+05  \n",
            "max     6.682700e+06  5.249075e+06  1.738700e+06  \n",
            "\n",
            "Missing Values:\n",
            "Region            0\n",
            "Country           0\n",
            "Item Type         0\n",
            "Sales Channel     0\n",
            "Order Priority    0\n",
            "Order Date        0\n",
            "Order ID          0\n",
            "Ship Date         0\n",
            "Units Sold        0\n",
            "Unit Price        0\n",
            "Unit Cost         0\n",
            "Total Revenue     0\n",
            "Total Cost        0\n",
            "Total Profit      0\n",
            "dtype: int64\n",
            "\n",
            "Data dengan Units Sold < 0:\n",
            "Empty DataFrame\n",
            "Columns: [Region, Country, Item Type, Sales Channel, Order Priority, Order Date, Order ID, Ship Date, Units Sold, Unit Price, Unit Cost, Total Revenue, Total Cost, Total Profit]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('100000 Sales Records.csv')\n",
        "\n",
        "# 1. Eksplorasi awal dataset\n",
        "print(\"Info Dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nStatistik Deskriptif:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 2. Tangani missing values\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())  # Isi missing values numerik dengan mean\n",
        "df[\"Total Cost\"] = df[\"Total Cost\"].fillna(df[\"Total Cost\"].mean())\n",
        "df[\"Total Profit\"] = df[\"Total Profit\"].fillna(df[\"Total Profit\"].mean())\n",
        "\n",
        "# 3. Hapus duplikasi\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 4. Periksa nilai tidak masuk akal (contoh: Units Sold < 0)\n",
        "print(\"\\nData dengan Units Sold < 0:\")\n",
        "print(df[df['Units Sold'] < 0])\n",
        "\n",
        "# Hapus data yang tidak masuk akal\n",
        "df = df[df['Units Sold'] >= 0]\n",
        "\n",
        "# 5. Standarisasi Kategori\n",
        "df['Region'] = df['Region'].str.title()  # Ubah menjadi title case\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "if (df['Units Sold'] == 0).any():\n",
        "    raise ValueError(\"Kolom 'Units Sold' memiliki nilai nol.\")\n",
        "\n",
        "# Feature Engineering sebelum normalisasi\n",
        "df['Revenue per Unit'] = df['Total Revenue'] / df['Units Sold']\n",
        "\n",
        "df['Log Total Revenue'] = np.log1p(df['Total Revenue'])  # log1p untuk menghindari log(0)\n",
        "\n",
        "# Normalisasi kolom numerik (setelah fitur baru dibuat)\n",
        "scaler = MinMaxScaler()\n",
        "df[['Total Profit', 'Log Total Revenue', 'Revenue per Unit']] = scaler.fit_transform(df[['Total Profit', 'Log Total Revenue', 'Revenue per Unit']])\n",
        "\n",
        "print(\"\\nStatistik Setelah Transformasi Log:\")\n",
        "print(df['Log Total Revenue'].describe())\n",
        "print(\"\\nStatistik Deskriptif Setelah Normalisasi:\")\n",
        "print(df[['Total Profit', 'Total Revenue', 'Revenue per Unit']].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mqJ8i-N8JJP",
        "outputId": "415efdbb-fbae-4493-d86d-d445f0934162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistik Setelah Transformasi Log:\n",
            "count    100000.000000\n",
            "mean          0.811823\n",
            "std           0.120497\n",
            "min           0.000000\n",
            "25%           0.750844\n",
            "50%           0.832341\n",
            "75%           0.898585\n",
            "max           1.000000\n",
            "Name: Log Total Revenue, dtype: float64\n",
            "\n",
            "Statistik Deskriptif Setelah Normalisasi:\n",
            "        Total Profit  Total Revenue  Revenue per Unit\n",
            "count  100000.000000   1.000000e+05     100000.000000\n",
            "mean        0.226656   1.336067e+06          0.390588\n",
            "std         0.218324   1.471768e+06          0.329226\n",
            "min         0.000000   1.866000e+01          0.000000\n",
            "25%         0.055154   2.797533e+05          0.151683\n",
            "50%         0.163141   7.898916e+05          0.298009\n",
            "75%         0.326900   1.836490e+06          0.649331\n",
            "max         1.000000   6.682700e+06          1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvRDsbn-PPZ_",
        "outputId": "d510528a-1b55-4a81-e3ed-0625ad3e24f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "_qTerygyx-Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql.functions import col, when, log1p\n",
        "\n",
        "# Membuat sesi Spark\n",
        "spark = SparkSession.builder.appName(\"MLExample\").getOrCreate()\n",
        "\n",
        "# Misalnya df adalah DataFrame Pandas yang sudah dinormalisasi sebelumnya\n",
        "# Jika dataset dalam bentuk Pandas, perlu mengonversinya ke Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "# Membuat kolom target jika diperlukan (misalnya target = 1 jika profit > 0, else 0)\n",
        "spark_df = spark_df.withColumn(\"target\", when(col(\"Total Profit\") > 0, 1).otherwise(0))\n",
        "\n",
        "# --- Memastikan fitur yang sudah dihitung tersedia di Spark DataFrame ---\n",
        "# Data seperti 'Revenue per Unit' dan 'Log Total Revenue' sudah ada setelah normalisasi\n",
        "# Pastikan kolom-kolom yang dibutuhkan ada, seperti 'Total Profit', 'Log Total Revenue', 'Revenue per Unit'\n",
        "\n",
        "# Tambahkan kolom 'Log Total Revenue' yang dihitung sebelumnya\n",
        "spark_df = spark_df.withColumn(\"Log Total Revenue\", log1p(col(\"Total Revenue\")))\n",
        "\n",
        "# Menampilkan skema kolom untuk memastikan kolom yang diperlukan ada\n",
        "spark_df.printSchema()\n",
        "\n",
        "# Menampilkan distribusi label target untuk memeriksa keseimbangan kelas\n",
        "spark_df.groupBy(\"target\").count().show()\n",
        "\n",
        "# Membuat VectorAssembler untuk menggabungkan fitur\n",
        "assembler = VectorAssembler(inputCols=[\"Total Profit\", \"Log Total Revenue\", \"Revenue per Unit\"], outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "# Transformasi data menjadi format yang bisa digunakan untuk model\n",
        "assembled_data = assembler.transform(spark_df)\n",
        "\n",
        "# Menampilkan hasil untuk memastikan\n",
        "assembled_data.select(\"features\").show(5)\n",
        "\n",
        "# Membagi data menjadi training set dan testing set\n",
        "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# Membuat model Logistic Regression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"target\")  # Pastikan 'target' adalah kolom label kamu\n",
        "\n",
        "# Melatih model menggunakan training data\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Membuat prediksi pada testing set\n",
        "predictions = lr_model.transform(test_data)\n",
        "\n",
        "# Menghitung evaluasi model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "# Menampilkan akurasi model\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# --- Cross-Validation untuk menghindari overfitting ---\n",
        "# Membuat parameter grid untuk hyperparameter tuning\n",
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).addGrid(lr.elasticNetParam, [0.0, 0.5]).build()\n",
        "\n",
        "# CrossValidator untuk tuning hyperparameter\n",
        "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
        "\n",
        "# Melatih model dengan cross-validation\n",
        "cv_model = crossval.fit(train_data)\n",
        "\n",
        "# Evaluasi hasil cross-validation\n",
        "cv_predictions = cv_model.transform(test_data)\n",
        "accuracy_cv = evaluator.evaluate(cv_predictions)\n",
        "\n",
        "# Menampilkan akurasi setelah cross-validation\n",
        "print(f\"Accuracy after Cross Validation: {accuracy_cv}\")\n",
        "\n",
        "# --- Memeriksa korelasi antara fitur dan target ---\n",
        "print(\"Korelasi antara Total Profit dan target:\", spark_df.corr(\"Total Profit\", \"target\"))\n",
        "print(\"Korelasi antara Log Total Revenue dan target:\", spark_df.corr(\"Log Total Revenue\", \"target\"))\n",
        "print(\"Korelasi antara Revenue per Unit dan target:\", spark_df.corr(\"Revenue per Unit\", \"target\"))\n"
      ],
      "metadata": {
        "id": "J4BLtMgSWDnP",
        "outputId": "d3e9042f-c187-41eb-86d9-8d1874d964f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Item Type: string (nullable = true)\n",
            " |-- Sales Channel: string (nullable = true)\n",
            " |-- Order Priority: string (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Order ID: long (nullable = true)\n",
            " |-- Ship Date: string (nullable = true)\n",
            " |-- Units Sold: long (nullable = true)\n",
            " |-- Unit Price: double (nullable = true)\n",
            " |-- Unit Cost: double (nullable = true)\n",
            " |-- Total Revenue: double (nullable = true)\n",
            " |-- Total Cost: double (nullable = true)\n",
            " |-- Total Profit: double (nullable = true)\n",
            " |-- Revenue per Unit: double (nullable = true)\n",
            " |-- Log Total Revenue: double (nullable = true)\n",
            " |-- target: integer (nullable = false)\n",
            "\n",
            "+------+-----+\n",
            "|target|count|\n",
            "+------+-----+\n",
            "|     1|99999|\n",
            "|     0|    1|\n",
            "+------+-----+\n",
            "\n",
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[0.02961757793565...|\n",
            "|[0.45509848943159...|\n",
            "|[0.01383879145509...|\n",
            "|[0.13141593916421...|\n",
            "|[0.55837361900318...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Accuracy: 1.0\n",
            "Accuracy after Cross Validation: 1.0\n",
            "Korelasi antara Total Profit dan target: 0.003283002431902958\n",
            "Korelasi antara Log Total Revenue dan target: 0.021305444880542496\n",
            "Korelasi antara Revenue per Unit dan target: 0.003751709468975074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**"
      ],
      "metadata": {
        "id": "cZiJAvoJyE75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Pastikan 'features' dan 'label' adalah nama kolom yang benar\n",
        "# dan bahwa assembled_data sudah didefinisikan sebelumnya\n",
        "\n",
        "# Membagi data menjadi training set dan testing set (jika belum dilakukan)\n",
        "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=1234)\n",
        "\n",
        "# Buat model Linear Regression\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"target\") # Ganti 'label' dengan nama kolom target\n",
        "\n",
        "# Latih model, gunakan train_data yang sudah didefinisikan\n",
        "lrModel = lr.fit(train_data)\n",
        "\n",
        "# Buat prediksi\n",
        "predictions = lrModel.transform(test_data)\n",
        "\n",
        "# Evaluasi model\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\", metricName=\"rmse\") # Ganti 'label' dengan nama kolom target\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wTfDBUg-5Qv",
        "outputId": "d69f65e9-ae32-4396-b727-8468b6adbb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) on test data = 0.000142738\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}